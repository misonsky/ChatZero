#coding=utf-8
import argparse
import os
import string
from trainer import Trainer
parser = argparse.ArgumentParser('parameters config for generate conversation')
string_settings = parser.add_argument_group('string settings')
string_settings.add_argument('--data_dir',type=str,default="datasets",help='dataset path')
string_settings.add_argument('--corpus',type=str,default="DailyDialog",help='select task to train')
string_settings.add_argument('--dict',type=str,default="dictionary",help='dictionary path')
string_settings.add_argument('--lang',type=str,default="de",help='which language to use')
string_settings.add_argument('--train_files',type=str,default="train.txt",help='train files')
string_settings.add_argument('--dev_files',type=str,default="dev.txt",help='the dev file evaluating the model')
string_settings.add_argument('--test_files',type=str,default="test.txt",help='the test file')
string_settings.add_argument('--train_features',type=str,default="train_%s.features",help='the train tfrecorder file')
string_settings.add_argument('--dev_features',type=str,default="dev_%s.features",help='the dev tfrecorder file')
string_settings.add_argument('--test_features',type=str,default="test_%s.features",help='the test tfrecorder file')
string_settings.add_argument('--model_dir',type=str,default="TrainModel",help='path to save model')
string_settings.add_argument('--process_path',type=str,default="process_path",help='save the pretrain Model')
string_settings.add_argument("--fp16_opt_level",type=str,default="O1",help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].")
string_settings.add_argument("--glove",type=str,default="glove",help="glove vectors")
string_settings.add_argument("--tokenizer_path",type=str,default="%s_beam.pkl",help="save the object of dataset")
string_settings.add_argument("--cache_dir",type=str,default="",help="Where do you want to store the pre-trained models downloaded from s3")
string_settings.add_argument("--config_name", default="", type=str, help="Pretrained config name or path if not the same as model_name")
string_settings.add_argument('--tokenizer_name',type=str,default="",help='Pretrained tokenizer name or path if not the same as model_name')
string_settings.add_argument('--model',type=str,default="bert",help='bert/gpt2/bart')
string_settings.add_argument('--model_name_or_path',type=str,default="pretrained",help='Path to pretrained model or model identifier from huggingface.co/models')
string_settings.add_argument('--zero_path',type=str,default="zero",help='Path to pretrained model or model identifier from huggingface.co/models')

boolean_settings = parser.add_argument_group('boolean settings')
boolean_settings.add_argument('--do_lower_case',type=bool,default=False,help='whether employ lower case')
boolean_settings.add_argument('--load_last_ckpt',type=bool,default=False,help='whether training the model from the last checkpoint')
boolean_settings.add_argument('--no_cuda',type=bool,default=False,help='whether use the cuda device')
boolean_settings.add_argument('--do_prepare',type=bool,default=False,help='prepare the dataset for supervised training')
boolean_settings.add_argument('--do_train',type=bool,default=True,help='train the supervised model')
boolean_settings.add_argument('--do_eval',type=bool,default=False,help='Whether to run eval on the dev set.')
boolean_settings.add_argument('--do_predict',type=bool,default=False,help='prediction the result for supervised')
boolean_settings.add_argument('--zero_setting',type=bool,default=True,help='whether using zero shot setting')
boolean_settings.add_argument('--fp16',type=bool,default=False,help='using the mixed_float16 when traing')
boolean_settings.add_argument('--drop_last',type=bool,default=False,help='whether drop the last dataset')

scaler_settings = parser.add_argument_group('scaler settings')
scaler_settings.add_argument("--local_rank",type=int,default=-1,help="local_rank for distributed training on gpus")
scaler_settings.add_argument('--max_turn',type=int,default=9,help='max number turn of conversation')
scaler_settings.add_argument('--max_utterance_len',type=int,default=512,help='max length of utterance')
scaler_settings.add_argument('--max_decode_length',type=int,default=50,help='length for decoder')
scaler_settings.add_argument('--max_examples',type=int,default=2,help='max tokens number')
scaler_settings.add_argument('--min_decode_length',type=int,default=1,help='length for decoder')
scaler_settings.add_argument('--beam_size',type=int,default=6,help="beam size of decoder")
scaler_settings.add_argument('--eval_steps',type=int,default=1000,help='number steps eval the model')
scaler_settings.add_argument('--log_steps',type=int,default=50,help='number steps log info')
scaler_settings.add_argument('--encoder_layers',type=int,default=1,help='the number of encoder layers')
scaler_settings.add_argument('--decoder_layers',type=int,default=1,help='the number of encoder layers')
scaler_settings.add_argument('--num_heads',type=int,default=8,help='head number of multi-head attention')
scaler_settings.add_argument('--kl_annealing_iter',type=int,default=2500,help="kl_annealing_iter")
scaler_settings.add_argument('--gradient_accumulation_steps',type=int,default=1,help='gradient accumulation steps')
scaler_settings.add_argument('--per_gpu_train_batch_size',type=int,default=4,help='Batch size per GPU/TPU core/CPU for training.')
scaler_settings.add_argument('--per_gpu_eval_batch_size',type=int,default=1,help='Batch size per GPU/TPU core/CPU for evaluation.')
scaler_settings.add_argument('--d_model',type=int,default=512,help='the hidden size of model')
scaler_settings.add_argument('--dff',type=int,default=1024,help='the hidden size of model')
scaler_settings.add_argument('--emb_size',type=int,default=512,help='the embedding dimension')
scaler_settings.add_argument('--repetition_penalty', type=float, default=1.0)
scaler_settings.add_argument('--length_penalty', type=float, default=1.0)
scaler_settings.add_argument('--no_repeat_ngram_size', type=int, default=0)
scaler_settings.add_argument('--warmup_steps',type=int,default=10000,help='Linear warmup over warmup_steps.')
scaler_settings.add_argument('--num_train_epochs',type=int,default=5000,help='Total number of training epochs to perform.')
scaler_settings.add_argument('--negatives_num',type=int,default=50,help='the negatives number when training.')
scaler_settings.add_argument('--dropout',type=float,default=0.0,help='dropout rate')
scaler_settings.add_argument('--learning_rate',type=float,default=5e-5,help='The initial learning rate for Adam.')
scaler_settings.add_argument('--weight_decay',type=float,default=0,help='Weight decay if we apply some.')
scaler_settings.add_argument('--adam_beta1',type=float,default=0.9,help='Beta1 for Adam optimizer')
scaler_settings.add_argument('--adam_beta2',type=float,default=0.999,help='Beta2 for Adam optimizer')
scaler_settings.add_argument('--adam_epsilon',type=float,default=1e-8,help='Epsilon for Adam optimizer.')
scaler_settings.add_argument('--max_grad_norm',type=float,default=1.0,help='Max gradient norm.')
scaler_settings.add_argument('--temperature',type=float,default=0.03,help='the temperature value when contrastive learning')
scaler_settings.add_argument('--alpha',type=float,default=0.6,help="alpha for length penalty")
scaler_settings.add_argument('--seed',type=int,default=12345,help="random seed")
scaler_settings.add_argument('--teach_rate',type=float,default=0.5,help="teach force")
config=parser.parse_args()
def run():
    config.model_name_or_path = os.path.join(config.model_name_or_path,config.model)
    trainer_handle = Trainer(config)
    if config.do_prepare:
        trainer_handle.prepare()
    elif config.do_train:
        trainer_handle.train()
    elif config.do_predict:
        trainer_handle.predictions()
if __name__=="__main__":
    run()
